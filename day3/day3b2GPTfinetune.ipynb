{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-qSg1EV2TH5",
        "outputId": "5d52c676-1a74-44b6-9382-1d6c860c6539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==1.60.0 (from -r requirements.txt (line 1))\n",
            "  Downloading openai-1.60.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.60.0->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.60.0->-r requirements.txt (line 1)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.60.0->-r requirements.txt (line 1)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.60.0->-r requirements.txt (line 1)) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.60.0->-r requirements.txt (line 1)) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai==1.60.0->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai==1.60.0->-r requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai==1.60.0->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai==1.60.0->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.60.0->-r requirements.txt (line 1)) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.60.0->-r requirements.txt (line 1)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.60.0->-r requirements.txt (line 1)) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.60.0->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.60.0->-r requirements.txt (line 1)) (2.27.2)\n",
            "Downloading openai-1.60.0-py3-none-any.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.1/456.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.59.6\n",
            "    Uninstalling openai-1.59.6:\n",
            "      Successfully uninstalled openai-1.59.6\n",
            "Successfully installed openai-1.60.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TRlMGkz-3x60"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from openai import OpenAI\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p36Jz56g5q7J"
      },
      "outputs": [],
      "source": [
        "# authenticate openai api keys\n",
        "ashu_api_key = \"\"\n",
        "client = OpenAI(api_key=ashu_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LFcWtgMf2_tY"
      },
      "outputs": [],
      "source": [
        "# location of data\n",
        "ashu_datapath = \"/content/dataset.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK9AlyNl3pNS",
        "outputId": "789e6d49-5623-4aef-a16a-720eaf87b318"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of examples we have in dataset is  22\n",
            "we can use this data for fine tuning\n"
          ]
        }
      ],
      "source": [
        "# creating list\n",
        "dataset = []\n",
        "# verify that my dataset is having a valid jsonl format\n",
        "with open(ashu_datapath, 'r') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            data = json.loads(line)\n",
        "            dataset.append(data)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Invalid JSONL format: {e}\")\n",
        "            break\n",
        "\n",
        "# check basic info\n",
        "# length of dataset\n",
        "if len(dataset) > 10 :\n",
        "  print(\"number of examples we have in dataset is \",len(dataset))\n",
        "  print(\"we can use this data for fine tuning\")\n",
        "else :\n",
        "  print(\"we need more data for fine tuning OR minimum 10 samples as per GPT\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEWdWQHI4F9j",
        "outputId": "3761d7fb-2741-4fcc-92af-5a04a7b7396d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found\n"
          ]
        }
      ],
      "source": [
        "# data format validation\n",
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "\n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wISFiETh7PC9",
        "outputId": "53c5b204-dcca-4fe4-809c-0e563c3ef1ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file-Lp2rUNgvUek6ps1hEtcjWZ\n"
          ]
        }
      ],
      "source": [
        "# start fine tuning process\n",
        "# step 1 -- uploading file using openAI\n",
        "ashu_data_file = client.files.create(\n",
        "  file=open(ashu_datapath, \"rb\"),\n",
        "  purpose='fine-tune'\n",
        ")\n",
        "# printing file id after upload\n",
        "myfile_id = ashu_data_file.id\n",
        "print(myfile_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U2JO5ZmU8r4a"
      },
      "outputs": [],
      "source": [
        "# setting suffix name of my model\n",
        "ashu_model_sufix = \"ashu_model-day3333\"\n",
        "# create a job to fine tune gpt4o-mini model using above file id\n",
        "ashu_job = client.fine_tuning.jobs.create(\n",
        "    training_file=myfile_id,\n",
        "    model=\"gpt-4o-mini-2024-07-18\",\n",
        "    suffix=ashu_model_sufix,\n",
        "    hyperparameters={\n",
        "        \"n_epochs\": 4\n",
        "        }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0ejGH4RAFBv",
        "outputId": "d8558483-1dd9-41ba-fdcd-07cd75f13719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The job has successfully completed\n",
            "New fine-tuned model created\n",
            "Checkpoint created at step 66\n",
            "Checkpoint created at step 44\n",
            "Step 88/88: training loss=0.00\n",
            "Step 87/88: training loss=0.02\n",
            "Step 86/88: training loss=0.06\n",
            "Step 85/88: training loss=0.05\n",
            "Step 84/88: training loss=0.15\n",
            "Step 83/88: training loss=0.02\n",
            "Step 82/88: training loss=0.07\n",
            "Step 81/88: training loss=0.03\n",
            "Step 80/88: training loss=0.08\n",
            "Step 79/88: training loss=0.08\n",
            "Step 78/88: training loss=0.15\n",
            "Step 77/88: training loss=0.03\n",
            "Step 76/88: training loss=0.04\n",
            "Step 75/88: training loss=0.20\n",
            "Step 74/88: training loss=0.29\n",
            "Step 73/88: training loss=0.11\n",
            "Step 72/88: training loss=0.08\n",
            "Step 71/88: training loss=0.01\n",
            "Step 70/88: training loss=0.09\n",
            "Step 69/88: training loss=0.19\n",
            "Step 68/88: training loss=0.27\n",
            "Step 67/88: training loss=0.22\n",
            "Step 66/88: training loss=0.20\n",
            "Step 65/88: training loss=0.09\n",
            "Step 64/88: training loss=0.09\n",
            "Step 63/88: training loss=0.17\n",
            "Step 62/88: training loss=0.32\n",
            "Step 61/88: training loss=0.48\n",
            "Step 60/88: training loss=0.27\n",
            "Step 59/88: training loss=0.06\n",
            "Step 58/88: training loss=0.31\n",
            "Step 57/88: training loss=0.02\n",
            "Step 56/88: training loss=0.19\n",
            "Step 55/88: training loss=0.20\n",
            "Step 54/88: training loss=0.54\n",
            "Step 53/88: training loss=0.20\n",
            "Step 52/88: training loss=0.13\n",
            "Step 51/88: training loss=0.51\n",
            "Step 50/88: training loss=0.10\n",
            "Step 49/88: training loss=0.42\n",
            "Step 48/88: training loss=0.14\n",
            "Step 47/88: training loss=0.12\n",
            "Step 46/88: training loss=0.41\n",
            "Step 45/88: training loss=0.44\n",
            "Step 44/88: training loss=0.24\n",
            "Step 43/88: training loss=1.14\n",
            "Step 42/88: training loss=1.00\n",
            "Step 41/88: training loss=0.63\n",
            "Step 40/88: training loss=0.14\n",
            "Step 39/88: training loss=0.35\n",
            "Step 38/88: training loss=0.83\n",
            "Step 37/88: training loss=0.77\n",
            "Step 36/88: training loss=0.47\n",
            "Step 35/88: training loss=0.86\n",
            "Step 34/88: training loss=1.02\n",
            "Step 33/88: training loss=0.59\n",
            "Step 32/88: training loss=0.52\n",
            "Step 31/88: training loss=0.44\n",
            "Step 30/88: training loss=0.52\n",
            "Step 29/88: training loss=0.44\n",
            "Step 28/88: training loss=0.59\n",
            "Step 27/88: training loss=0.21\n",
            "Step 26/88: training loss=0.37\n",
            "Step 25/88: training loss=0.57\n",
            "Step 24/88: training loss=1.10\n",
            "Step 23/88: training loss=0.47\n",
            "Step 22/88: training loss=0.93\n",
            "Step 21/88: training loss=0.80\n",
            "Step 20/88: training loss=1.54\n",
            "Step 19/88: training loss=1.56\n",
            "Step 18/88: training loss=1.98\n",
            "Step 17/88: training loss=0.96\n",
            "Step 16/88: training loss=1.92\n",
            "Step 15/88: training loss=1.02\n",
            "Step 14/88: training loss=1.37\n",
            "Step 13/88: training loss=1.77\n",
            "Step 12/88: training loss=0.93\n",
            "Step 11/88: training loss=1.32\n",
            "Step 10/88: training loss=2.01\n",
            "Step 9/88: training loss=1.35\n",
            "Step 8/88: training loss=1.05\n",
            "Step 7/88: training loss=2.19\n",
            "Step 6/88: training loss=2.30\n",
            "Step 5/88: training loss=3.72\n",
            "Step 4/88: training loss=3.33\n",
            "Step 3/88: training loss=4.50\n",
            "Step 2/88: training loss=3.23\n",
            "Step 1/88: training loss=2.83\n",
            "Fine-tuning job started\n",
            "Files validated, moving job to queued state\n",
            "Validating training file: file-Lp2rUNgvUek6ps1hEtcjWZ\n",
            "Created fine-tuning job: ftjob-sx0rSqhW19VIW4hySZmC5hpZ\n"
          ]
        }
      ],
      "source": [
        "# list events of fine tuning progress\n",
        "ashu_job_events = client.fine_tuning.jobs.list_events(fine_tuning_job_id=ashu_job.id)\n",
        "for event in ashu_job_events:\n",
        "    print(event.message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hTHDlIdB3ya",
        "outputId": "459eeaa4-632f-424d-c2c7-d7f113db55ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ft:gpt-4o-mini-2024-07-18:delvex:ashu-model-day3333:AsYf5gRO:ckpt-step-44\n",
            "ft:gpt-4o-mini-2024-07-18:delvex:ashu-model-day3333:AsYf5WOh:ckpt-step-66\n",
            "ft:gpt-4o-mini-2024-07-18:delvex:ashu-model-day3333:AsYf6kGy\n"
          ]
        }
      ],
      "source": [
        "# list all my fine tuned models in my account\n",
        "ashu_models = client.models.list()\n",
        "for model in ashu_models.data:\n",
        "    if 'day33' in model.id:\n",
        "      print(model.id)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
